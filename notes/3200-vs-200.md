# Some of Brandon's observations on 200 vs 3200 models

## `black_mated_in_1`
Working on a modified version of the test position which allows white to play
`E2` moves, which forces black to not play any `NE` moves.
```
ww.. wwww
.... .w..
.... ....
bbbb bbbb

.... ..ww
.b.. ....
..b. .w..
wwww .b..
```
With this test position, it appears that both the 3200 and the 200 models are
able to spot and avoid the mate in 1. Both models' top candidate move was
`c2f1N2`, which works because in the resulting board state, white is unable to
play `SW2` on their homeboard. This was tested
- searching with 400 and 800 simulations
- with and without dirichlet search (i.e., both `s 400` and `n 400`)
- with and without 1.5x exploration bonus
So I'm unable to reproduce what Thomas mentioned earlier about neither net being
able to avoid mate in 1.

As for why neither one likes the "intended" avoidance `f1b3E2`, not sure.

## `material_advantage`, `material_disadvantage`
Two new test positions with very straightforward setups. Black either has a large material advantage or disadvantage, but neither side has an immediate win.
Here's what `material_advantage` looks like:
```
w... w...
.... ....
.... ....
bbbb bbbb

w... w...
.... ....
.... ....
bbbb bbbb
```

We compare the behavior of the two models on these test positions.

### 200 checkpoint:
```
> b material_advantage
> s 800
> p
B(visits: 800, avg_reward: +0.348, prior: +0.000, value: +0.065)+
> c
  a1e1N: W(visits: 763, avg_reward: -0.347, prior: +0.004, value: -0.186)+
h1d5NW2: W(visits:  37, avg_reward: -0.369, prior: +0.008, value: -0.483)+
  a1f1N: W(visits:   0, avg_reward: +0.000, prior: +0.004, value: None)
  a1g1N: W(visits:   0, avg_reward: +0.000, prior: +0.004, value: None)
  a1h1N: W(visits:   0, avg_reward: +0.000, prior: +0.005, value: None)
> s 400
> c
  a1e1N: W(visits: 400, avg_reward: -0.338, prior: +0.004, value: -0.186)+
  a1f1N: W(visits:   0, avg_reward: +0.000, prior: +0.004, value: None)
  a1g1N: W(visits:   0, avg_reward: +0.000, prior: +0.004, value: None)
```

Some things that I found interesting here:
1. The value net's evaluation of the position is `0.065`, which is very close
   to drawing. See the 3200 eval below for comparison.
2. Priors (policy net outputs) are roughly uniformly distributed between
   candidate moves.
3. Search really is not exploratory at all, even with exploration bonus.
   I think the priors being so small kills the entire term and lets Q term
	 dominate.
4. The second search, with all 400 sims only being used to consider a single
   move, is pretty concerning to me (however I'm not so worried -- see 3200)
5. The sign of the children's `value` is a bit hard to interpret, so let me
   clarify: it's the value net's output run on those positions from the child's
	 POV. so `value=-0.186` means that in the position after `a1e1N` is applied
	 and the board is flipped, the value net thinks that position is losing.
	 Negative evaluations for child nodes means roughly that our current position
	 is better (although `value` at such early stages is a pretty noisy output)

Now for `material_disadvantage`:
```
> b material_disadvantage
> s 800
> p
B(visits: 800, avg_reward: -0.360, prior: +0.000, value: -0.357)+
> c
e1a5NE2: W(visits:  75, avg_reward: +0.333, prior: +0.044, value: +0.016)+
 a1e5NE: W(visits:  74, avg_reward: +0.329, prior: +0.042, value: +0.234)+
 e1a5N2: W(visits:  70, avg_reward: +0.336, prior: +0.047, value: +0.112)+
  a1e1N: W(visits:  66, avg_reward: +0.336, prior: +0.039, value: +0.175)+
  e1a1N: W(visits:  62, avg_reward: +0.330, prior: +0.033, value: +0.175)+
```
1. Here there are less legal moves in general, so priors are bigger, the Q term
   in UCB is not so dominant, and we see more exploration of all the different
   candidate moves. **This makes me curious if instead of scaling exploration
   bonus by a fixed constant like 1.5, we scale it by some factor of the number
   of legal moves i.e. child nodes.**
2. Value is `-0.357`, so at least its not fence-sitting like in the
   `material_advantage` position, but it could still fully be a coincidence
3. Priors are still roughly uniformly distributed.


### 3200 checkpoint:

```
> b material_advantage
> s 800
> p
B(visits: 800, avg_reward: +0.894, prior: +0.000, value: +0.871)+
> c
c1h1NW2: W(visits: 339, avg_reward: -0.905, prior: +0.296, value: -0.880)+
b1e1NE2: W(visits: 167, avg_reward: -0.891, prior: +0.204, value: -0.883)+
 b1h1N2: W(visits:  67, avg_reward: -0.879, prior: +0.099, value: -0.893)+
> s 400
> c
c1h1NW2: W(visits: 171, avg_reward: -0.903, prior: +0.296, value: -0.880)+
b1e1NE2: W(visits:  92, avg_reward: -0.888, prior: +0.204, value: -0.883)+
 b1h1N2: W(visits:  43, avg_reward: -0.885, prior: +0.099, value: -0.893)+
b1f1NE2: W(visits:  39, avg_reward: -0.883, prior: +0.092, value: -0.869)+
> b material_disadvantage
> s 800
> p
B(visits: 800, avg_reward: -0.894, prior: +0.000, value: -0.883)+
> c
 e1a5NE: W(visits: 185, avg_reward: +0.882, prior: +0.174, value: +0.824)+
 e1a5N2: W(visits: 136, avg_reward: +0.881, prior: +0.126, value: +0.837)+
  e1a5E: W(visits:  75, avg_reward: +0.885, prior: +0.076, value: +0.834)+
 e1a5E2: W(visits:  71, avg_reward: +0.884, prior: +0.071, value: +0.883)+
```
1. Look at the `value`s for each of the positions: `0.871` for
   `material_advantage`, and `-0.833` for `material_disadvantage`. I think this
   is a strong enough difference to say that the value net has learned to prefer
   more material.
2. `value`s for child nodes are also all strongly negative or positive depending
   on the position, which again makes sense since if black has material
   advantage then white is at a material disadvantage and vice-versa.
3. **Unlike 200**, `s 400` on `material_advantage` is exploring more nodes now.
   If my theory is right, it's due to priors being not so tiny. The other good
	 sign about this is we maybe don't need to worry if a certain checkpoint is
	 not exploring some subtrees well -- it may automatically learn to explore
	 them later
	 - This I don't fully understand conceptually, since it seemed to me that if
	   we never explore some moves in MCTS, and we use MCTS visits distribution to
		 train policy net, how does that help the situation? But somehow it works,
		 maybe something to do with how cross entropy loss works
4. As covered in the previous point, priors are now less uniformly distributed.
   There appears to be a better concept of a "plan" with certain moves being
	 weighted higher.

## `marginalization`, `centralization`
These two test positions are a bit iffier, but my theory is that stones in the
center 4 squares of each sub-board may be valued higher than stones around the
edges, since pieces in the center are not in _as_ imminent risk of being pushed
off, and have more mobility because they can move in all 8 directions. However
with these test positions the advantage really isn't clear. _Maybe we can refine
these positions later_

`centralization` looks like this:
```
..w. ..w.
.... ....
.b.. .b..
.... ....

.... ....
.... ....
.b.. ..b.
...w w...
```
And `marginalization` is the same setup except piece colors are flipped.

### `200 checkpoint`
```
> b centralization
> s 800
> p
B(visits: 800, avg_reward: +0.114, prior: +0.000, value: +0.018)+
> c
 g2b6NE: W(visits: 157, avg_reward: -0.164, prior: +0.028, value: +0.013)+
 b2g2N2: W(visits: 108, avg_reward: -0.156, prior: +0.037, value: -0.229)+
 g2b2N2: W(visits: 107, avg_reward: -0.157, prior: +0.035, value: -0.229)+
> b marginalization
> s 800
> p
B(visits: 800, avg_reward: -0.075, prior: +0.000, value: +0.052)+
> c
 d1e1N2: W(visits: 473, avg_reward: +0.034, prior: +0.155, value: -0.067)+
 e1d1N2: W(visits: 197, avg_reward: +0.059, prior: +0.150, value: -0.067)+
 d1g8W2: W(visits:  35, avg_reward: +0.217, prior: +0.162, value: +0.142)+
```

200 likes the margins more actually.

### `3200 checkpoint`
```
> b centralization
> s 800
> p
B(visits: 800, avg_reward: +0.072, prior: +0.000, value: +0.088)+
> c
  b2g2N: W(visits:  95, avg_reward: -0.095, prior: +0.065, value: -0.227)+
 b2g2NW: W(visits:  92, avg_reward: -0.095, prior: +0.066, value: -0.013)+
  b2g2W: W(visits:  89, avg_reward: -0.114, prior: +0.026, value: +0.077)+
> b marginalization
> s 800
> p
B(visits: 800, avg_reward: -0.080, prior: +0.000, value: -0.168)+
> c
 d1e1N2: W(visits: 415, avg_reward: +0.050, prior: +0.312, value: +0.191)+
 d1g8W2: W(visits: 117, avg_reward: +0.077, prior: +0.113, value: +0.256)+
 e1d1N2: W(visits:  86, avg_reward: +0.136, prior: +0.214, value: +0.191)+
```
The 3200 net is as usual a bit more opinionated on both value and policy,
though not by much in this case. It does seem to dislike the edges a bit.

Curiously the candidate moves preferred by both networks in the
`marginalization` position are the same. `d1e1n2` (or equivalently `e1d1n2`)
seems like a pretty good move to me since it takes two black pieces out of the
corner while keeping it out of harms way. But the 200 net plays randomly besides
some assistance from MCTS, so either
- this behavior is coincidental, or
- MCTS is able to see why other options are bad.
